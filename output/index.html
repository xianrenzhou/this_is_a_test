<h2 id="hi-.-here-is-the-homepage-of-intelligent-cognitive-systems-laboratory-icost-beijing-university-of-posts-and-telecommunications.">Hi üëã. Here is the homepage of Intelligent Cognitive Systems Laboratory (iCOST), Beijing University of Posts and Telecommunications.</h2>
<!--

**Here are some ideas to get you started:**

üôã‚Äç‚ôÄÔ∏è A short introduction - what is your organization all about?
üåà Contribution guidelines - how can the community get involved?
üë©‚Äçüíª Useful resources - where can the community find your docs? Is there anything else the community should know?
üçø Fun facts - what does your team eat for breakfast?
üßô Remember, you can do mighty things with the power of [Markdown](https://docs.github.com/github/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax)
-->
<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#papers">Papers</a>
<ul>
<li><a href="#2d-human-pose-estimation">2D Human Pose Estimation</a></li>
<li><a href="#3d-human-pose-estimation">3D Human Pose Estimation</a></li>
<li><a href="#3d-human-motion-prediction">3D Human Motion Prediction</a></li>
<li><a href="#early-action-prediction">Early Action Prediction</a></li>
<li><a href="#skeleton-based-human-action-recognition">Skeleton-based Human Action Recognition</a></li>
<li><a href="#group-activity-recognition">Group Activity Recognition</a></li>
<li><a href="#uncertainty-aware-scene-understanding-with-point-clouds">Uncertainty-aware Scene Understanding with Point Clouds</a></li>
<li><a href="#audio-visual-learning">Audio-Visual Learning</a></li>
<li><a href="#audio-and-speech-processing">Audio and Speech Processing</a></li>
<li><a href="#medical-image-segmentation">Medical Image Segmentation</a></li>
<li><a href="#adversarial-attack">Adversarial Attack</a></li>
<li><a href="#others">Others</a></li>
</ul></li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>The Intelligent Cognitive Systems Laboratory (iCost) at BUPT (Beijing University of Posts and Telecommunications) is actively engaged in long-term research in multiple cutting-edge fields, including computer vision and embodied intelligence. Our research spans various sub-domains such as action recognition, human pose prediction and estimation, uncertainty research, multimodal and audio-visual learning, audio-visual event detection, medical image segmentation, 3D object detection, adversarial strategies, embodied navigation, and robot grasping.</p>
<p>Our research team has achieved substantial results, publishing numerous high-quality research papers in internationally recognized and authoritative journals such as IEEE Transactions on Image Processing (IEEE TIP), IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT), and top-tier conferences like AAAI.</p>
<p>In the Intelligent Cognitive Systems Laboratory, we encourage communication and collaboration among team members, fostering a rigorous and harmonious academic atmosphere. We warmly welcome scholars, researchers, and students who are passionate about artificial intelligence and related fields to join us or collaborate. We believe that everyone with a curiosity for science can find their stage here.</p>
<h2 id="papers">Papers</h2>
<h3 id="d-human-pose-estimation">2D Human Pose Estimation</h3>
<p>[<strong>PR 2024</strong>] Kinematics Modeling Network for Video-based Human Pose Estimation [<a href="https://arxiv.org/pdf/2207.10971.pdf">paper</a>] [<a href="https://github.com/YHDang/KIMNet">code</a>]</p>
<p>[<strong>TIP 2022</strong>] Relation-Based Associative Joint Location for Human Pose Estimation in Videos [<a href="https://ieeexplore.ieee.org/document/9786543">paper</a>] [<a href="https://github.com/YHDang/pose-estimation">code</a>]</p>
<p>[<strong>KBS 2024</strong>] DHRNet: A Dual-Path Hierarchical Relation Network for Multi-Person Pose Estimation [<a href="https://github.com/YHDang/DHRNet">code</a>]</p>
<p>[-] BiHRNet: A Binary high-resolution network for Human Pose Estimation [<a href="https://arxiv.org/abs/2311.10296">paper</a>]</p>
<h3 id="d-human-pose-estimation-1">3D Human Pose Estimation</h3>
<p>[<strong>AAAI 2024</strong>] Lifting by Image - Leveraging Image Cues for Accurate 3D Human Pose Estimation [<a href="https://arxiv.org/abs/2312.15636">paper</a>]</p>
<h3 id="d-human-motion-prediction">3D Human Motion Prediction</h3>
<p>[<strong>KBS 2024</strong>] April-GCN: Adjacency Position-velocity Relationship Interaction Learning GCN for Human motion prediction [<a href="https://authors.elsevier.com/sd/article/S0950-7051(24)00248-X">paper</a>]</p>
<p>[<strong>TNNLS 2023</strong>] Learning Constrained Dynamic Correlations in Spatiotemporal Graphs for Motion Prediction [<a href="https://ieeexplore.ieee.org/abstract/document/10138910">paper</a>] [<a href="https://github.com/Jaakk0F/DSTD-GCN">code</a>]</p>
<p>[<strong>TCSVT 2023</strong>] Collaborative Multi-Dynamic Pattern Modeling for Human Motion Prediction [<a href="https://ieeexplore.ieee.org/abstract/document/10025861">paper</a>]</p>
<p>[<strong>TCSVT 2022</strong>] Towards more realistic human motion prediction with attention to motion coordination [<a href="https://ieeexplore.ieee.org/abstract/document/9745623/">paper</a>]</p>
<p>[<strong>TCSVT 2021</strong>] TrajectoryCNN: a new spatio-temporal feature learning network for human motion prediction [<a href="https://ieeexplore.ieee.org/document/9186039">paper</a>] [<a href="https://github.com/lily2lab/TrajectoryCNN">code</a>]</p>
<p>[<strong>Neurocomputing 2024</strong>] Physics-constrained Attack against Convolution-based Human Motion Prediction [<a href="https://arxiv.org/abs/2306.11990">paper</a>] [<a href="https://github.com/ChengxuDuan/advHMP">code</a>]</p>
<p>[<strong>Neurocomputing 2022</strong>] Temporal consistency two-stream CNN for human motion prediction [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231221014892?via%3Dihub">paper</a>]</p>
<p>[<strong>Êú∫Âô®‰∫∫ 2022</strong>] Èù¢Âêë‰∫∫‰ΩìÂä®‰ΩúÈ¢ÑÊµãÁöÑÂØπÁß∞ÊÆãÂ∑ÆÁΩëÁªú [<a href="https://robot.sia.cn/cn/article/doi/10.13973/j.cnki.robot.210188#:~:text=%E6%91%98%E8%A6%81%3A%20%E4%B8%BA%E4%BA%86%E7%A0%94%E7%A9%B6%E4%B8%8D%E5%90%8C%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E6%96%B9%E5%BC%8F%E5%AF%B9%E4%BA%BA%E4%BD%93%E5%8A%A8%E4%BD%9C%E9%A2%84%E6%B5%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%BD%B1%E5%93%8D%EF%BC%8C%E6%8E%A2%E8%AE%A8%E4%BA%86%E5%9C%A8%E4%BF%9D%E6%8C%81%E7%BD%91%E7%BB%9C%E6%B7%B1%E5%BA%A6%E4%B8%80%E5%AE%9A%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%EF%BC%8C%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E6%9E%84%E6%88%90%E4%B8%80%E4%B8%AA%E9%AB%98%E6%95%88%E6%8D%95%E6%8D%89%E4%BA%BA%E4%BD%93%E5%8A%A8%E4%BD%9C%E7%89%B9%E5%BE%81%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E3%80%82%20%E9%80%9A%E8%BF%87%E8%A7%82%E5%AF%9F%E4%BA%BA%E4%BD%93%E9%AA%A8%E9%AA%BC%E5%85%B3%E8%8A%82%E7%82%B9%E6%8E%92%E5%88%97%E6%96%B9%E5%BC%8F%EF%BC%8C%E6%8F%90%E5%87%BA%E4%B8%80%E7%A7%8D%E9%80%82%E7%94%A8%E4%BA%8E%E4%BA%BA%E4%BD%93%E9%AA%A8%E9%AA%BC%E5%85%B3%E8%8A%82%E7%82%B9%E9%A2%84%E6%B5%8B%E7%9A%84%E5%AF%B9%E7%A7%B0%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E6%96%B9%E6%B3%95%EF%BC%8C%E5%B9%B6%E5%9F%BA%E4%BA%8E%E8%AF%A5%E6%96%B9%E6%B3%95%E8%AE%BE%E8%AE%A1%E4%BA%86%E5%AF%B9%E7%A7%B0%E6%AE%8B%E5%B7%AE%E5%9D%97,%28symmetric%20residual%20block%EF%BC%8CSRB%29%E3%80%82">paper</a>]</p>
<p>[<strong>MPE 2020</strong>] A Hierarchical Static-Dynamic Encoder-Decoder Structure for 3D Human Motion Prediction with Residual CNNs [<a href="https://www.hindawi.com/journals/mpe/2020/7064910/">paper</a>] [<a href="https://github.com/liujin0/SDnet">code</a>]</p>
<p>[<strong>Cognitive Computation and Systems 2020</strong>] Stacked residual blocks based encoder‚Äìdecoder framework for human motion prediction[<a href="https://github.com/lily2lab/residual_prediction_network">code</a>]</p>
<p>[-]Uncertainty-aware Human Motion Prediction [<a href="https://scholar.google.com/scholar?oi=bibs&amp;cluster=11543079145147482533&amp;btnI=1&amp;hl=en">paper</a>]</p>
<p>[-] MSSL: Multi-scale Semi-decoupled Spatiotemporal Learning for 3D human motion prediction [<a href="https://arxiv.org/abs/2010.05133">paper</a>][<a href="https://github.com/lily2lab/MSSL">code</a>]</p>
<p>[-] DeepSSM: Deep State-Space Model for 3D Human Motion Prediction [<a href="https://arxiv.org/abs/2005.12155">paper</a>] [<a href="https://github.com/lily2lab/DeepSSM">code</a>]</p>
<h3 id="early-action-prediction">Early Action Prediction</h3>
<p>[<strong>TIP 2024</strong>] Rich Action-semantic Consistent Knowledge for Early Action Prediction [<a href="https://www.semanticscholar.org/reader/7ec7b4929c73ade2c926b65e88bdefaa03148115">paper</a>] [<a href="https://github.com/lily2lab/RACK">code</a>]</p>
<p>[<strong>ICCSIP 2022</strong>] A discussion of data sampling strategies for early action prediction [<a href="https://link.springer.com/chapter/10.1007/978-981-16-9247-5_24">paper</a>]</p>
<p>[<strong>‰∏≠ÂõΩËá™Âä®ÂåñÂ§ß‰ºö 2023</strong>] An end-to-end multi-scale network for action prediction in videos</p>
<h3 id="skeleton-based-human-action-recognition">Skeleton-based Human Action Recognition</h3>
<p>[<strong>TCSVT 2024</strong>] SiT-MLP: A Simple MLP with Point-wise Topology Feature Learning for Skeleton-based Action Recognition [<a href="https://ieeexplore.ieee.org/document/10495051">paper</a>] [<a href="https://github.com/BUPTSJZhang/SiT-MLP">code</a>]</p>
<p>[<strong>RAS 2020</strong>] DWnet: Deep-wide network for 3D action recognition [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0921889019308176">paper</a>] [<a href="https://github.com/YHDang/DWnet">code</a>]</p>
<p>[-] Spatial-Temporal Decoupling Contrastive Learning for Skeleton-based Human Action Recognition [<a href="https://arxiv.org/abs/2312.15144">paper</a>] [<a href="https://github.com/BUPTSJZhang/STD-CL">code</a>]</p>
<h3 id="group-activity-recognition">Group Activity Recognition</h3>
<p>[<strong>KBS 2024</strong>] MLP-AIR: An effective MLP-based module for actor interaction relation learning in group activity recognition [<a href="https://www.sciencedirect.com/science/article/pii/S0950705124010876">paper</a>]</p>
<h3 id="uncertainty-aware-scene-understanding-with-point-clouds">Uncertainty-aware Scene Understanding with Point Clouds</h3>
<p>[<strong>TGRS 2023</strong>] Neighborhood Spatial Aggregation MC Dropout for Efficient Uncertainty-aware Semantic Segmentation in Point Clouds [<a href="https://ieeexplore.ieee.org/document/10247069/">paper</a>] [<a href="https://github.com/chaoqi7/Uncertainty_Estimation_PCSS">code</a>]</p>
<p>[<strong>TCSVT 2023</strong>] Instance-incremental Scene Graph Generation from Real-world Point Clouds via Normalizing Flows [<a href="https://ieeexplore.ieee.org/document/10164228/">paper</a>] [<a href="https://github.com/chaoqi7/GPL3D">code</a>]</p>
<p>[<strong>TIM 2020</strong>] Multigranularity Semantic Labeling of Point Clouds for the Measurement of the Rail Tanker Component With Structure Modeling [<a href="https://ieeexplore.ieee.org/document/9207911/">paper</a>] [<a href="https://github.com/chaoqi7/Multi-granularity-Semantic-Labeling-with-Structure-Modeling-TIM">code</a>]</p>
<p>[<strong>ICRA 2021</strong>] Neighborhood Spatial Aggregation based Efficient Uncertainty Estimation for Point Cloud Semantic Segmentation [<a href="https://ieeexplore.ieee.org/document/9560972/">paper</a>] [<a href="https://github.com/chaoqi7/Uncertainty_Estimation_PCSS">code</a>]</p>
<p>[<strong>Tsinghua Science and Technology 2023</strong>] Dynamic Scene Graph Generation of Point Clouds with Structural Representation Learning [<a href="https://ieeexplore.ieee.org/document/10225283/">paper</a>]</p>
<h3 id="audio-visual-learning">Audio Visual Learning</h3>
<p>[<strong>TMM 2023</strong>] Leveraging the Video-level Semantic Consistency of Event for Audio-visual Event Localization [<a href="https://ieeexplore.ieee.org/abstract/document/10286391">paper</a>] [<a href="https://github.com/Bravo5542/VSCG">code</a>]</p>
<p>[<strong>EMNLP 2023</strong>] Target-Aware Spatio-Temporal Reasoning via Answering Questions in Dynamic Audio-Visual Scenarios [<a href="https://aclanthology.org/2023.findings-emnlp.630/">paper</a>] [<a href="https://github.com/Bravo5542/TJSTG">code</a>]</p>
<p>[<strong>ËÆ°ÁÆóÊú∫Â∫îÁî® 2021</strong>] Âü∫‰∫éÂÖ≥ÈîÆÂ∏ßÁ≠õÈÄâÁΩëÁªúÁöÑËßÜÂê¨ËÅîÂêàÂä®‰ΩúËØÜÂà´ [<a href="http://www.joca.cn/CN/10.11772/j.issn.1001-9081.2021060995">paper</a>]</p>
<p>[-] Past Future Motion Guided Network for Audio Visual Event Localization [<a href="https://arxiv.org/abs/2205.03802v1">paper</a>]</p>
<h3 id="audio-and-speech-processing">Audio and Speech Processing</h3>
<p>[<strong>ICPR 2024</strong>] Full-frequency dynamic convolution: a physical frequency-dependent convolution for sound event detection [<a href="https://arxiv.org/abs/2401.04976">paper</a>] [<a href="https://github.com/Harper812/FFDConv">code</a>]</p>
<p>[<strong>Interspeech 2024</strong>] MFF-EINV2: Multi-scale Feature Fusion across Spectral-Spatial-Temporal Domains for Sound Event Localization and Detection [<a href="https://arxiv.org/pdf/2406.08771">paper</a>] [<a href="https://github.com/muuda/MFF-EINV2">code</a>]</p>
<h3 id="medical-image-segmentation">Medical Image Segmentation</h3>
<p>[<strong>IEEE-CYBER 2023</strong>] Multi-task Learning Network for CT Whole Heart Segmentation [<a href="https://ieeexplore.ieee.org/document/10256432">paper</a>]</p>
<p>[<strong>Biomedical Signal Processing and Control 2022</strong>] DC-net: Dual-Consistency Semi-Supervised Learning for 3D Left Atrium Segmentation from MRI [<a href="https://www.sciencedirect.com/science/article/abs/pii/S1746809422003858">paper</a>]</p>
<h3 id="adversarial-attack">Adversarial Attack</h3>
<p>[<strong>Neurocomputing 2023</strong>] Physics-constrained attack against convolution-based human motion prediction [<a href="https://www.sciencedirect.com/science/article/pii/S0925231224000432?via%3Dihub">paper</a>]</p>
<h3 id="others">Others</h3>
<p>[<strong>MTAP2023</strong>] Transfer the global knowledge for current gaze estimation [<a href="https://link.springer.com/article/10.1007/s11042-023-17484-2">paper</a>]</p>
<p>[<strong>TCSVT 2021</strong>] Energy-based Periodicity Mining with Deep Features for Action Repetition Counting in Unconstrained Videos [<a href="https://ieeexplore.ieee.org/document/9339959">paper</a>] [<a href="https://github.com/BUPT-COST-lab/ActionCounting">code</a>]</p>
<p>[<strong>ROBIO 2019</strong>]DBNet: A New Generalized Structure Efficient for Classification [<a href="https://ieeexplore.ieee.org/abstract/document/8961680/">paper</a>] [<a href="https://github.com/YHDang/DBNet">code</a>]</p>
<p>[-] SDVRF: Sparse-to-Dense Voxel Region Fusion for Multi-modal 3D Object Detection [<a href="https://arxiv.org/abs/2304.08304">paper</a>]</p>
<h2 id="last-update-august-22-2024">Last update: August 22, 2024</h2>
<p>Feel free to contact us at 7858833@bupt.edu.cn, or zsj@bupt.edu.cn.</p>
